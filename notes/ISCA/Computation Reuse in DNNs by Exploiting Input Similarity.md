**Paper title:**

Computation Reuse in DNNs by Exploiting Input Similarity

**Publication:**

ISCAâ€™18

**Problem to solve:**

1.  Traditional efficient support for DNNs on CPUs, GPUs and accelerators only
    focus on a single execution of a DNN, but ignore the similarity of
    successive frames of speech or images of a video.

2.  To the different types of DNNs, include MLPs, CNNs and RNNs, they have
    different computation part, include FC, convolutional and recurrent layers.
    Each type has different structure for computation.

3.  Reusing the data from previous layers may lead to loss on accuracy.

**Major contribution:**

A new architecture of DNNs accelerator with reusing data from previous layers is
proposed for majority of DNNs models. The author gets the idea that computation
reuse can save energy and improve speed with minimal impact in accuracy by
analyzing the data from each layers of three main kinds of DNNs network.

1.  Ruse some result of previous execution, instead of computing the entire DNN,
    the computation related to inputs with negligible changes can be avoided,
    which will save a large percentage of computations and memory access with
    minor loss on accuracy.

2.  Analyze the degree of similarity in the inputs of all neurons for
    consecutive execution of a DNN with neural network for speech recognition,
    video classification and self-driving cars. And the results show that more
    than 60% of inputs of any fully connected and convolutional layer have the
    same value in two consecutive, whereas more than 50% of the inputs of
    recurrent layers remain unmodified.

3.  An architecture (shown in Fig.1) with compute engine(CE), control unit(CU),
    SRAM and one specific eDRAM is proposed. In the accelerator, the CE contains
    the functional units and employ to quantize the inputs in reuse scheme. The
    CU provides the appropriate control signals in every cycle and stores the
    centroids of the clusters. And the eDRAM memory is used to store the
    synaptic weights of different layers.
